<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A simple yet effective V2A-Mapper for open-domain vision-to-audio generation.">
  <meta name="keywords" content="V2A-Mapper, Cross-modal generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/v2a.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.au/citations?user=jPj4ViQAAAAJ&hl=en">Heng Wang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1CSIbMsAAAAJ&hl=fr">Jianbo Ma</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7cVOyh0AAAAJ&hl=ca">Santiago Pascual</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Richard Cartwright<sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/tom-cai.html">Weidong Cai</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Sydney</span>
            <span class="author-block"><sup>2</sup>Dolby Laboratories</span><br>
            <p style="font-size:17px;">* Work done during an internship at Dolby</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/v2a-mapper_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.09300"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="todo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (coming soon)</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/heng-hw/V2A-Mapper"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Test Samples</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <!-- <span class="link-block">
                <a href="todo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Hugging Face Space (coming soon)</span>
                  </a>
              </span> -->
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg" width="719" height="245"
                 class="interpolation-image"
                 alt="A lightweight solution to utilize foundation models in vision-to-audio generation."/>
      <h2 class="subtitle has-text-centered">
        <b>V2A-Mapper</b> connects Foundation Models for vision-to-audio generation.
      </h2>
    </div>
  </div>
</section>
<!--/ teaser image -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Content. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Content</h2>
        <div class="content has-text-justified">
          <div class="toc" >
          <ul>
            <li>
              <a href="#abstract">Abstract</a>
            </li>
            <li>
              <a href="#method">Method</a>
            </li>
            <li>
              <a href="#more">Visually-guided Sound Generation Examples</a>
            </li>
            <li>
              <a href="#im2a-prior">Image-to-Audio Generation: Comparison with Prior Works</a>
            </li>
            <li>
              <a href="#vi2a-prior">Video-to-Audio Generation: Comparison with Prior Works</a>
            </li>
            <li>
              <a href="#variability">Variability of Our V2A Generation Model</a>
            </li>
            <li>
              <a href="#latent">Latent Space Interpolation</a>
            </li>
            <li>
              <a href="#bridge-gap">Domain Gap Bridging Process</a>
            </li>
            <li>
              <a href="#vta-bottleneck">Why Vision-Text-Audio is Bottlenecked by the Captioner</a>
            </li>
            
          </ul>
        </div>
        </div>
      </div>
    </div>
    <!--/ Content. -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. 
            Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. 
            However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. 
          </p>
          <p>
            On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. 
            To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. 

          </p>
          <p>
            In this project, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. 
            Then we propose a simple yet effective mapper mechanism <b>V2A-Mapper</b> to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with <b>86%</b> fewer parameters but achieving <b>53%</b> and <b>19%</b> improvement in FD and CS, respectively. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="method">Method</h2>
        <div class="column is-full-width">
          <img src="./static/images/method.png" 
                   class="interpolation-image"
                   alt="Interpolate start reference image."/><br><br>
                   
          <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <div class="content has-text-justified">
            <p>
              Our lightweight method only requires the training of a <b>V2A-Mapper</b> to bridge the domain gap 
              between the vision representative FM CLIP and the audio generative FM AudioLDM. 
              The <b>V2A-Mapper</b> is supervised by the audio representative FM CLAP to learn the translation 
              from visual space to auditory space. 
              Leveraging the generalization and knowledge transfer ability of foundation models, 
              the <b>V2A-Mapper</b> is trained with the same modestly sized dataset but the 
              overall system can achieve much better performance.
            </p>
          </div>
        </div>
      </div>
    </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Interpolation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="more">Visually-guided Sound Generation Examples</h2>

        <!-- Guided by image -->
        <div class="content has-text-justified">
          <p>Our V2A-Mapper helps synthesize high-fidelity and visually-relevant sound based on images/videos.</p>
        <h3 class="title is-4">Synthetic image-guided sound generation</h3>

          <p>
            The first three images are generated with <a href="https://www.midjourney.com/showcase/recent/">Midjourney</a> and the last one is a cat meme.
          </p>
          <div class="t" >
            <ul>
              <li>
                <img src="./static/image-to-sound/image/dog.png" />
                <audio controls>
                    <source src=./static/image-to-sound/audio/dog.wav type="audio/wav">
                  </audio>
              </li>
              <li>
                <img src="./static/image-to-sound/image/tiger.png" />
                <audio controls>
                    <source src=./static/image-to-sound/audio/tiger.wav type="audio/wav">
                  </audio>
              </li>
              <li>
                <img src="./static/image-to-sound/image/forest.png" />
                <audio controls>
                    <source src=./static/image-to-sound/audio/forest.wav type="audio/wav">
                  </audio>
              </li>
              <li>
                <img src="./static/image-to-sound/image/drum.png" />
                <audio controls>
                    <source src=./static/image-to-sound/audio/drum.wav type="audio/wav">
                  </audio>
              </li>
              
            </ul>
            </div>
        </div>
        <br/>
        <!--/ Guided by image -->

        <!-- Guided by text -->
        <div class="content has-text-justified">
        <h3 class="title is-4">Video-guided sound generation</h3>

          <ul>
            <li>
              <iframe src="./static/video-to-sound/rel_17_p.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
        
            <li>
              <iframe src="./static/video-to-sound/carrace_ours.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
            <li>
              <iframe src="./static/video-to-sound/rel_14_p.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
            <li>
              <iframe src="./static/video-to-sound/waterbubble.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
          </ul>
          <ul>
            <li>
              <iframe src="./static/video-to-sound/game_sound.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
        
            <li>
              <iframe src="./static/video-to-sound/snow_walk.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
            <li>
              <iframe src="./static/video-to-sound/cartoon_bird.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
            <li>
              <iframe src="./static/video-to-sound/steel.mp4" frameborder="0" allowfullscreen></iframe>
            </li>
          </ul>

        </div>
    </div>
</section>

<!-- <a href="https://pages.cs.huji.ac.il/adiyoss-lab/im2wav/"> </a>-->
<!-- <a href="https://salu133445.github.io/clipsonic/"></a> -->
<!-- <a href="https://text-to-audio.github.io/"> --></a>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Image-to-Audio Generation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="im2a-prior">Image-to-Audio Generation: Comparison with Prior Works</h2>
        
        <div class="content has-text-justified">
          <p>
            We compare our method with previous works including Im2Wav<sup id="fnref:im2wav" role="doc-noteref"><a href="#fn:im2wav" class="footnote" rel="footnote">1</a></sup>, 
             CLIPSonic-IQ<sup id="fnref:clipsonic" role="doc-noteref"><a href="#fn:clipsonic" class="footnote" rel="footnote">2</a></sup>, and 
             Make-An-Audio<sup id="fnref:makeanaudio" role="doc-noteref"><a href="#fn:makeanaudio" class="footnote" rel="footnote">3</a></sup>. 
             We generate 10-second audio clips for all the evaluation. Our method can synthesize different categories of audio sound from in-the-wild images.
          </p>
        </div>
        <!-- ImageHear -->
        <div class="content has-text-justified">
        <h3 class="title is-4">ImageHear</h3>

          <p>
            ImageHear<sup id="fnref:im2wav" role="doc-noteref"><a href="#fn:im2wav" class="footnote" rel="footnote">1</a></sup> contains 101 images from 30 visual classes (2-8 images per class). 
            We show comparison with Im2Wav and CLIPSonic-IQ with samples from ImageHear here.
          </p>
        </div>

          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/image-to-sound/image/piano3.jpg" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/piano3_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/piano3_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/piano3_ours.wav type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/image-to-sound/image/horse4_resize.png" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/horse4_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/horse4_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/horse4_ours.wav type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/image-to-sound/image/cat1.jpg" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/cat1_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/cat1_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/cat1_ours.wav type="audio/wav">
              </audio>
            </div>
            
            <div class="grid-item">
              <img src="./static/image-to-sound/image/rain4.jpg" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/rain4_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/rain4_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/rain4.wav type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/image-to-sound/image/bongo4.jpg" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/bongo4_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/bongo4_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/bongo4.wav type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/image-to-sound/image/car3.jpg" alt="main table" />
              Im2Wav
              <audio controls>
                <source src=./static/image-to-sound/audio/car3_im2wav.wav type="audio/wav">
              </audio>
              ClipSonic-IQ
              <audio controls>
                <source src=./static/image-to-sound/audio/car3_clipsonic.wav type="audio/wav">
              </audio>
              Ours
              <audio controls>
                <source src=./static/image-to-sound/audio/car3.wav type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
                <img src="./static/image-to-sound/image/train1.jpg" alt="main table" />
                Im2Wav
                <audio controls>
                  <source src=./static/image-to-sound/audio/train1_im2wav.wav type="audio/wav">
                </audio>
                ClipSonic-IQ
                <audio controls>
                  <source src=./static/image-to-sound/audio/train1_clipsonic.wav type="audio/wav">
                </audio>
                Ours
                <audio controls>
                  <source src=./static/image-to-sound/audio/train1.wav type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/image-to-sound/image/police1.jpg" alt="main table" />
                Im2Wav
                <audio controls>
                  <source src=./static/image-to-sound/audio/police1_im2wav.wav type="audio/wav">
                </audio>
                ClipSonic-IQ
                <audio controls>
                  <source src=./static/image-to-sound/audio/police1_clipsonic.wav type="audio/wav">
                </audio>
                Ours
                <audio controls>
                  <source src=./static/image-to-sound/audio/police1.wav type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/image-to-sound/image/guitar2_resize.png" alt="main table" />
                Im2Wav
                <audio controls>
                  <source src=./static/image-to-sound/audio/guitar2_im2wav.wav type="audio/wav">
                </audio>
                ClipSonic-IQ
                <audio controls>
                  <source src=./static/image-to-sound/audio/guitar2_clipsonic.wav type="audio/wav">
                </audio>
                Ours
                <audio controls>
                  <source src=./static/image-to-sound/audio/guitar2_ours.wav type="audio/wav">
                </audio>
              </div>


          </div>

        
            



        <br/>
        <!--/ ImageHear -->


        <!-- Comparison with Make-an-Audio. -->
        <div class="content has-text-justified">
        <h3 class="title is-4">Comparison with Make-An-Audio</h3>
          <p>
            We also compare with Make-An-Audio. 
            The audio clips generated by Make-An-Audio and the image samples are extracted from their <a href="https://text-to-audio.github.io/">project website</a>.

          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/image-to-sound/image/a+photo+of+race+car.png" alt="main table" />
              Ours
              <audio controls>
                <source src="./static/image-to-sound/audio/a+photo+of+race+car_ours.wav" type="audio/wav">
              </audio>
              Make-An-Audio
              <audio controls>
                <source src="./static/image-to-sound/audio/a+photo+of+race+car.wav" type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
                <img src="./static/image-to-sound/image/fireworks.png" alt="main table" />
                Ours
                <audio controls>
                  <source src="./static/image-to-sound/audio/fireworks_ours.wav" type="audio/wav">
                </audio>
                Make-An-Audio
                <audio controls>
                  <source src="./static/image-to-sound/audio/fireworks.wav" type="audio/wav">
                </audio>
              </div>

            <div class="grid-item">
            <img src="./static/image-to-sound/image/thunder+lightning.png" alt="main table" />
            Ours
            <audio controls>
                <source src="./static/image-to-sound/audio/thunder+lightning_ours.wav" type="audio/wav">
            </audio>
            Make-An-Audio
            <audio controls>
                <source src="./static/image-to-sound/audio/thunder+lightning.wav" type="audio/wav">
            </audio>
            </div>

            <div class="grid-item">
                <img src="./static/image-to-sound/image/wind.png" alt="main table" />
                Ours
                <audio controls>
                    <source src="./static/image-to-sound/audio/wind_ours.wav" type="audio/wav">
                </audio>
                Make-An-Audio
                <audio controls>
                    <source src="./static/image-to-sound/audio/wind.wav" type="audio/wav">
                </audio>
            </div>

            <div class="grid-item">
                <img src="./static/image-to-sound/image/bird.png" alt="main table" />
                Ours
                <audio controls>
                    <source src="./static/image-to-sound/audio/bird_ours.wav" type="audio/wav">
                </audio>
                Make-An-Audio
                <audio controls>
                    <source src="./static/image-to-sound/audio/bird.wav" type="audio/wav">
                </audio>
            </div>

            <div class="grid-item">
                <img src="./static/image-to-sound/image/alarm.png" alt="main table" />
                Ours
                <audio controls>
                    <source src="./static/image-to-sound/audio/alarm_ours.wav" type="audio/wav">
                </audio>
                Make-An-Audio
                <audio controls>
                    <source src="./static/image-to-sound/audio/alarm.wav" type="audio/wav">
                </audio>
            </div>


        </div>

        </div>

    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Video-to-Audio Generation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="vi2a-prior">Video-to-Audio Generation: Comparison with Prior Works</h2>
        
        <!-- VGGSound -->
        <div class="content has-text-justified">
          <p>
            We compare our method with previous works including Im2Wav<sup id="fnref:im2wav" role="doc-noteref"><a href="#fn:im2wav" class="footnote" rel="footnote">1</a></sup>, 
               CLIPSonic-IQ<sup id="fnref:clipsonic" role="doc-noteref"><a href="#fn:clipsonic" class="footnote" rel="footnote">2</a></sup>, and 
               Make-An-Audio<sup id="fnref:makeanaudio" role="doc-noteref"><a href="#fn:makeanaudio" class="footnote" rel="footnote">3</a></sup>. 
               We generate 10-second audio clips for all the evaluation. Our method can synthesize matching sound for videos captured in real-world scenarios.
              </p>
        <h3 class="title is-4">VGGSound</h3>

          <p>
            VGGSound<sup id="fnref:vggsound" role="doc-noteref"><a href="#fn:vggsound" class="footnote" rel="footnote">4</a></sup> contains 
            199,176 10-second video clips extracted from videos uploaded to YouTube with audio-visual correspondence. 
            Like Im2Wav and CLIPSonic-IQ, we train our V2A-Mapper with VGGSound only. Our method generalizes better to unseen videos compared to other approaches. Note the Foundation Models we use have never been trained on VGGSound.
            <font color="#a64d79">VGGSound
            contains noisy data whose audio and visual streams might
            not be highly-relevant. To observe how our method is robust to noisy training data, please watch the sports live video</font>.  
          </p>

          <div class="t" >
            <ul>
              <li>
                <iframe src="./static/video-to-sound/rel_7_p.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/rel_7_r.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_7_i.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_7_c.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>
            <ul>
              <li>
                <iframe src="./static/video-to-sound/disney_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/disney_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/disney_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/disney_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

            <ul>
              <li>
                <iframe src="./static/video-to-sound/football_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours <p style="color: #a64d79">(the sports live video)</p>
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/football_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth 
              </li>

              <li>
                <iframe src="./static/video-to-sound/football_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/football_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>


            <ul>
              <li>
                <iframe src="./static/video-to-sound/snowdrive_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/snowdrive_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/snowdrive_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/snowdrive_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

            <ul>
              <li>
                <iframe src="./static/video-to-sound/rel_6_p.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/rel_6_r.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_6_i.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_6_c.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>


            <ul>
              <li>
                <iframe src="./static/video-to-sound/sing_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/sing_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/sing_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/sing_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

            <ul>
              <li>
                <iframe src="./static/video-to-sound/baby-laugh_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/baby-laugh_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/baby-laugh_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/baby-laugh_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

            <ul>
              <li>
                <iframe src="./static/video-to-sound/rel_5_p.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/rel_5_r.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_5_i.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/rel_5_c.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

    

            <ul>
              <li>
                <iframe src="./static/video-to-sound/rock-concert_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/rock-concert_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/rock-concert_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/rock-concert_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

            <ul>
              <li>
                <iframe src="./static/video-to-sound/firetruck-paper_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
          
              <li>
                <iframe src="./static/video-to-sound/firetruck-paper_gt.mp4" frameborder="0" allowfullscreen></iframe>
                Ground Truth
              </li>

              <li>
                <iframe src="./static/video-to-sound/firetruck-paper_im2wav.mp4" frameborder="0" allowfullscreen></iframe>
                Im2Wav
              </li>

              <li>
                <iframe src="./static/video-to-sound/firetruck-paper_clipsonic.mp4" frameborder="0" allowfullscreen></iframe>
                ClipSonic-IQ
              </li>
            </ul>

           



            

            

        </div>
        <br/>
        <!--/ VGGSound -->

        <!-- Comparison with Make-an-Audio. -->
        <div class="content has-text-justified">
        <h3 class="title is-4">Comparison with Make-an-Audio</h3>
          <p>
            We also compare with Make-An-Audio. 
            The audio clips generated by Make-An-Audio and the video samples are extracted from their <a href="https://text-to-audio.github.io/">project website</a>.
          </p>


          <div class="t" >
            <ul>
              <li>
                <iframe src="./static/video-to-sound/railroad+car+train+wagon_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
              <li>
                <iframe src="./static/video-to-sound/railroad+car+train+wagon_0.mp4" frameborder="0" allowfullscreen></iframe>
                Make-An-Audio
              </li>
              <li>
                <iframe src="./static/video-to-sound/sheep_clip_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                Ours
              </li>
              <li>
                <iframe src="./static/video-to-sound/sheep_clip_0.mp4" frameborder="0" allowfullscreen></iframe>
                Make-An-Audio
              </li>
            </ul>

            <ul>
                <li>
                  <iframe src="./static/video-to-sound/typing_clip_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/typing_clip_0.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
                <li>
                  <iframe src="./static/video-to-sound/playing+snare+drum_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/playing+snare+drum_0.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
              </ul>

              <ul>
                <li>
                  <iframe src="./static/video-to-sound/church+bell_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/church+bell_0.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
                <li>
                  <iframe src="./static/video-to-sound/babycrying_0_ours_resize.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/babycrying_0_resize.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
              </ul>

              <ul>
                <li>
                  <iframe src="./static/video-to-sound/fireworks_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/fireworks_0.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
                <li>
                  <iframe src="./static/video-to-sound/violin_0_ours.mp4" frameborder="0" allowfullscreen></iframe>
                  Ours
                </li>
                <li>
                  <iframe src="./static/video-to-sound/violin_0.mp4" frameborder="0" allowfullscreen></iframe>
                  Make-An-Audio
                </li>
              </ul>


        </div>

        </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Variability -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="variability">Variability of Our V2A Generation Model</h2>

        <!-- Image-to-Audio -->
        <div class="content has-text-justified">
          <p>
            We present three audio clips generated from the same visual input to showcase the diversity of our method. Since our V2A-Mapper is a diffusion-based generative model, it is capable of modeling one-to-many relationships.
          </p>
        <h3 class="title is-4">Image-to-Audio</h3>

          <p>
            For the same image, our method is capable of generating different but visually-relevant audio clips.
          </p>
          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/variability_img2audio/guitar3.jpg" alt="main table" />
              Sample 1
              <audio controls>
                <source src="./static/variability_img2audio/guitar3.wav" type="audio/wav">
              </audio>
              Sample 2
              <audio controls>
                <source src="./static/variability_img2audio/guitar3_613.wav" type="audio/wav">
              </audio>
              Sample 3
              <audio controls>
                <source src="./static/variability_img2audio/guitar3_920.wav" type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
                <img src="./static/variability_img2audio/bird2.jpg" alt="main table" />
                Sample 1
                <audio controls>
                  <source src="./static/variability_img2audio/bird2.wav" type="audio/wav">
                </audio>
                Sample 2
                <audio controls>
                  <source src="./static/variability_img2audio/bird2_613.wav" type="audio/wav">
                </audio>
                Sample 3
                <audio controls>
                  <source src="./static/variability_img2audio/bird2_920.wav" type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/variability_img2audio/drums2.jpg" alt="main table" />
                Sample 1
                <audio controls>
                  <source src="./static/variability_img2audio/drums2.wav" type="audio/wav">
                </audio>
                Sample 2
                <audio controls>
                  <source src="./static/variability_img2audio/drums2_613.wav" type="audio/wav">
                </audio>
                Sample 3
                <audio controls>
                  <source src="./static/variability_img2audio/drums2_920.wav" type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/variability_img2audio/bells1.jpg" alt="main table" />
                Sample 1
                <audio controls>
                  <source src="./static/variability_img2audio/bells1.wav" type="audio/wav">
                </audio>
                Sample 2
                <audio controls>
                  <source src="./static/variability_img2audio/bells1_613.wav" type="audio/wav">
                </audio>
                Sample 3
                <audio controls>
                  <source src="./static/variability_img2audio/bells1_920.wav" type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/variability_img2audio/trumpet1.jpg" alt="main table" />
                Sample 1
                <audio controls>
                  <source src="./static/variability_img2audio/trumpet1.wav" type="audio/wav">
                </audio>
                Sample 2
                <audio controls>
                  <source src="./static/variability_img2audio/trumpet1_613.wav" type="audio/wav">
                </audio>
                Sample 3
                <audio controls>
                  <source src="./static/variability_img2audio/trumpet1_920.wav" type="audio/wav">
                </audio>
              </div>

              <div class="grid-item">
                <img src="./static/variability_img2audio/accordion1.jpg" alt="main table" />
                Sample 1
                <audio controls>
                  <source src="./static/variability_img2audio/accordion1.wav" type="audio/wav">
                </audio>
                Sample 2
                <audio controls>
                  <source src="./static/variability_img2audio/accordion1_ldm2023.wav" type="audio/wav">
                </audio>
                Sample 3
                <audio controls>
                  <source src="./static/variability_img2audio/accordion1_920.wav" type="audio/wav">
                </audio>
              </div>

        </div>
      </div>
        <br/>
        <!--/ Image-to-Audio -->

        <!-- Video-to-Audio -->
        <div class="content has-text-justified">
        <h3 class="title is-4">Video-to-Audio</h3>
          <p>
            For the same video, our method is capable of generating different but visually-relevant audio clips.
          </p>

          <div class="t" >
            <ul>
                <li>
                  <iframe src="./static/variability_vid2audio/penguine_6.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 1
                </li>
                <li>
                  <iframe src="./static/variability_vid2audio/penguine_888.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 2
                </li>
                <li>
                  <iframe src="./static/variability_vid2audio/penguine_920_ldm.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 3
                </li>
              </ul>
            <ul>
              <li>
                <iframe src="./static/variability_vid2audio/feather_6.mp4" frameborder="0" allowfullscreen></iframe>
                Sample 1
              </li>
              <li>
                <iframe src="./static/variability_vid2audio/feather_888.mp4" frameborder="0" allowfullscreen></iframe>
                Sample 2
              </li>
              <li>
                <iframe src="./static/variability_vid2audio/feather_1.mp4" frameborder="0" allowfullscreen></iframe>
                Sample 3
              </li>
            </ul>

            <ul>
                <li>
                  <iframe src="./static/variability_vid2audio/firetruck_920_ldm.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 1
                </li>
                <li>
                  <iframe src="./static/variability_vid2audio/firetruck_920.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 2
                </li>
                <li>
                  <iframe src="./static/variability_vid2audio/firetruck_6.mp4" frameborder="0" allowfullscreen></iframe>
                  Sample 3
                </li>
              </ul>

              

        </div>


        



        

        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Interpolation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="latent">Latent Space Interpolation</h2>

        <!-- Guided by image -->
        <div class="content has-text-justified">
          <p>
            Thanks to the V2A-Mapper, our method supports interpolation along the latent space guided by either visual or textual prompt.
            We show the process by interleaving ten 3-second intermediate audio clips with 1-second pause. We generate 10-second sound but just present the first three seconds to observe the transition.
          </p>
        <h3 class="title is-4">Guided by image</h3>

          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/interpolation_img/cat-to-lion.png" alt="main table" />
              cat to lion
              <audio controls>
                <source src="./static/interpolation_img/cat-to-lion_mixdown.wav" type="audio/wav">
              </audio>
            </div>
            <div class="grid-item">
                <img src="./static/interpolation_img/frog-to-flute.png" alt="main table" />
                frog to flute
                <audio controls>
                  <source src="./static/interpolation_img/frog-to-flute_mixdown.wav" type="audio/wav">
                </audio>
              </div>
              <div class="grid-item">
                <img src="./static/interpolation_img/bird-to-bell.png" alt="main table" />
                bird to bell
                <audio controls>
                  <source src="./static/interpolation_img/bird-to-bell_mixdown.wav" type="audio/wav">
                </audio>
              </div>
        </div>
        </div>
        <br/>
        <!--/ Guided by image -->

        <!-- Guided by text -->
        <div class="content has-text-justified">
        <h3 class="title is-4">Guided by text</h3>

        <div class="grid-container">
          <div class="grid-item">
            <img src="./static/interpolation_txt/rock.png" alt="main table" />
            guitar to rock style
            <audio controls>
              <source src="./static/interpolation_txt/gutar-to-rock_guitar_mixdown.wav" type="audio/wav">
            </audio>
          </div>
          <div class="grid-item">
              <img src="./static/interpolation_txt/drum.png" alt="main table" />
              guitar to drum
              <audio controls>
                <source src="./static/interpolation_txt/guitar-to-drum_mixdown.wav" type="audio/wav">
              </audio>
            </div>
            <div class="grid-item">
              <img src="./static/interpolation_txt/baby.png" alt="main table" />
              bird to crying baby
              <audio controls>
                <source src="./static/interpolation_txt/bird-to-cry_babies_mixdown.wav" type="audio/wav">
              </audio>
            </div>
      </div>
        </div>
    </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Interpolation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="bridge-gap">Supplementary Demo 1: Domain Gap Bridging Process</h2>

        <!-- Guided by image -->
        <div class="content has-text-justified">
          <p>
          In this section, we demonstrate the proposed V2A-Mapper can bridge the domain gap between vision and audio spaces. If the mapper is absent (w/o the mapper), directly conditioning the audio generator with CLIP embeddings (i.e., the visual features) would lead to random and nonsense results. The examples below demonstrate the CLIP space is not understandable by the audio generator. After placing the proposed V2A-Mapper in between (w/ the mapper), we can observe that the generated sound makes much more sense. With the mapper, the vision space has been converted into the corresponding auditory space which can be interpreted by the audio generator.
          </p>

          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/domain_gap_audio-demo/frog1.jpg" alt="main table" />
              w/o the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/frog1_wo_mapper.wav" type="audio/wav">
              </audio>
              w/ the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/frog1_proposed.wav" type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/domain_gap_audio-demo/sheep1.jpg" alt="main table" />
              w/o the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/sheep1_wo_mapper.wav" type="audio/wav">
              </audio>
              w/ the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/sheep1_proposed.wav" type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/domain_gap_audio-demo/train2.jpg" alt="main table" />
              w/o the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/train2_wo_mapper.wav" type="audio/wav">
              </audio>
              w/ the mapper
              <audio controls>
                <source src="./static/domain_gap_audio-demo/train2_proposed.wav" type="audio/wav">
              </audio>
            </div>

        </div>
        </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Interpolation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" id="vta-bottleneck">Supplementary Demo 2: Why Vision-Text-Audio is Bottlenecked by the Captioner</h2>

        <!-- Guided by image -->
        <div class="content has-text-justified">
          <p>
          In this section, we demonstrate why using a captioner to generate description and then apply text-to-audio generator is not an optimal solution to the vision-to-audio synthesis problem. Given available image-to-text (e.g. BLIP) and text-to-audio (e.g. AudioLDM) generation systems, it is intuitive to adopt a vision-text-audio pipeline to solve the vision-to-audio synthesis task without any training. However, we observe that this intuitive solution is bottlenecked by the performance of the captioner. As shown below, if BLIP fails at predicting the object (<font color="#ff0000">drum set</font> instead of <font color="#2AAA8A">bongo</font>; <font color="#ff0000">flute</font> instead of <font color="#2AAA8A">harmonica</font>), the sound output would also be wrong.
          </p>

          <div class="grid-container">
            <div class="grid-item">
              <img src="./static/vision_text_audio_demo/bongo2.jpg" alt="main table" />
              vision-text-audio w/ BLIP-generated captions
              <i>"a man sitting in front of a <font color="#ff0000">drum set</font>"</i>
              <audio controls>
                <source src="./static/vision_text_audio_demo/bongo2_txt-based.wav" type="audio/wav">
              </audio>
              w/ mapper
              <audio controls>
                <source src="./static/vision_text_audio_demo/bongo2_proposed.wav" type="audio/wav">
              </audio>
            </div>

            <div class="grid-item">
              <img src="./static/vision_text_audio_demo/harmonica2.jpg" alt="main table" />
              vision-text-audio w/ BLIP-generated captions
              <i>"a man with dreadlocks playing a <font color="#ff0000">flute</font>"</i>
              <audio controls>
                <source src="./static/vision_text_audio_demo/harmonica2_txt-based.wav" type="audio/wav">
              </audio>
              w/ mapper
              <audio controls>
                <source src="./static/vision_text_audio_demo/harmonica2_proposed.wav" type="audio/wav">
              </audio>
            </div>

        </div>
        </div>

</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{v2a-mapper,
  title     = {V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models},
  author    = {Wang, Heng and Ma, Jianbo and Pascual, Santiago and Cartwright, Richard and Cai, Weidong},
  booktitle   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2024},
}</code></pre>
  </div>
</section>

<section class="section" id="References">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <div class="ordlist">
      <ol>
        <li id="fn:im2wav" role="doc-endnote"><p>Roy Sheffer and Yossi Adi, "I Hear Your True Colors: Image Guided Audio Generation," Proc. ICASSP, 2023. <a href="#fnref:im2wav" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li>
        <li id="fn:clipsonic" role="doc-endnote"><p>Hao-Wen Dong et al., "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models," arXiv preprint arXiv:2306.09635, 2023.<a href="#fnref:clipsonic" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li>
        <li id="fn:makeanaudio" role="doc-endnote"><p>Rongjie Huang et al., "Make-An-Audio: Text-to-Audio Generation with Prompt-enhanced Diffusion Models," Proc. ICML, 2023.<a href="#fnref:makeanaudio" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li>
        <li id="fn:vggsound" role="doc-endnote"><p>Honglie Chen et al., "VGGSound: A Large-scale Audio-Visual Dataset," Proc. ICASSP, 2020<a href="#fnref:vggsound" class="reversefootnote" role="doc-backlink">&#8617;</a></p></li>

      </ol>
    </div>
    
    
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/v2a-mapper_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/heng-hw/V2A-Mapper" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
            Source code is mainly borrowed from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
